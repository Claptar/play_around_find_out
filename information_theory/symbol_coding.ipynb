{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e2bc08",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda5709",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f78e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import heapq\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d13a9a",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, prefix=\"\", is_last=True):\n",
    "    \"\"\"Print tree structure as text\"\"\"\n",
    "    if node is None:\n",
    "        return\n",
    "    \n",
    "    # Print current node\n",
    "    connector = \"└── \" if is_last else \"├── \"\n",
    "    if node.symbol:\n",
    "        print(f\"{prefix}{connector}{node.symbol} ({node.weight})\")\n",
    "    else:\n",
    "        print(f\"{prefix}{connector}* ({node.weight if node.weight else 'N/A'})\")\n",
    "    \n",
    "    # Update prefix for children\n",
    "    new_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "    \n",
    "    # Print children\n",
    "    if node.left or node.right:\n",
    "        if node.left:\n",
    "            print_tree(node.left, new_prefix, not bool(node.right))\n",
    "        if node.right:\n",
    "            print_tree(node.right, new_prefix, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d247e",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fc59d",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "This notebook is inpired by information theory [lectures by David McKay](https://www.youtube.com/playlist?list=PLN3p8NUNcClDu1hc2m5cVp8FOEmuF3vRy). I want to implement [Huffman coding algorithm](https://en.wikipedia.org/wiki/Huffman_coding) for data compression and use it to compress [human genome](https://en.wikipedia.org/wiki/Human_genome) sequence. Why doing it? Well, I'm greately facinated by [Arithmetic coding](https://en.wikipedia.org/wiki/Arithmetic_coding) algorithm for data compression and Huffman's algorithm would be great as a beseline to compare to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da187cfc",
   "metadata": {},
   "source": [
    "## Brief overview of related concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4f91c",
   "metadata": {},
   "source": [
    "### Symbol coding problem\n",
    "\n",
    "In symbol coding problem we have a set of symbols $S = \\{s_1, s_2, \\ldots, s_n\\}$ with probabilities $P = \\{p_1, p_2, \\ldots, p_n\\}$ and we want to encode them using binary strings (codewords) $C = \\{c_1, c_2, \\ldots, c_n\\}$ such that the expected codeword length is minimized. The expected codeword length $L$ is given by:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i), \\tag{1}\n",
    "$$\n",
    "\n",
    "where $l(c_i)$ is the length of codeword $c_i$. Let's look at an example to illustrate this.\n",
    "\n",
    "#### Example 1\n",
    "Let's say we have a set of symbols $S = \\{A, B, C, D\\}$ with probabilities $P = \\{0.5, 0.25, 0.125, 0.125\\}$. We want to find the optimal codewords $C$ that minimize the expected codeword length.\n",
    "\n",
    "a) One possible solution is to assign the following codewords:\n",
    "- A: 1000\n",
    "- B: 0100\n",
    "- C: 0010\n",
    "- D: 0001\n",
    "\n",
    "The expected codeword length $L$ is calculated as follows:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i) = 0.5 \\cdot 4 + 0.25 \\cdot 4 + 0.125 \\cdot 4 + 0.125 \\cdot 4 = 4\n",
    "$$\n",
    "\n",
    "b) We can do much better if we consider only codewords of size 2:\n",
    "\n",
    "-A: 00\n",
    "-B: 01\n",
    "-C: 10\n",
    "-D: 11\n",
    "\n",
    "The expected codeword length $L$ is calculated as follows:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i) = 0.5 \\cdot 2 + 0.25 \\cdot 2 + 0.125 \\cdot 2 + 0.125 \\cdot 2 = 2\n",
    "$$\n",
    "\n",
    "c) Another possible solution is to assign the following codewords:\n",
    "- A: 1\n",
    "- B: 01\n",
    "- C: 001\n",
    "- D: 0001\n",
    "\n",
    "The expected codeword length $L$ is calculated as follows:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i) = 0.5 \\cdot 1 + 0.25 \\cdot 2 + 0.125 \\cdot 3 + 0.125 \\cdot 4 = 1.875\n",
    "$$\n",
    "\n",
    "f) Finally we can try to go for even shorter expected length:\n",
    "- A: 1\n",
    "- B: 00\n",
    "- C: 010\n",
    "- D: 10\n",
    "\n",
    "The expected codeword length $L$ is calculated as follows:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i) = 0.5 \\cdot 1 + 0.25 \\cdot 2 + 0.125 \\cdot 3 + 0.125 \\cdot 4 = 1.625\n",
    "$$\n",
    "\n",
    "\n",
    "What is important to mention is that we want our coding to have several useful properties:\n",
    "- **Uniquely decodable**: for any string $x$ and $y$ such that $x \\neq y$ codewords $C(x)$ and $C(y)$ must be different $C(x) \\neq C(y)$. In plain english this means that we can always decode a string of codewords back to the original symbols without ambiguity. In this light, solution (c) in example 1 is not uniquely decodable because the both string `DC` and `ABD` are encoded as $C(DC)=C(ABD) = 10010$.\n",
    "- **Minimal expected codeword length**: we want to minimize the expected codeword length $L$.\n",
    "\n",
    "**Note:** [ASCII](https://en.wikipedia.org/wiki/ASCII) code is another interesting example of symbol coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665a49f",
   "metadata": {},
   "source": [
    "### Source coding theorem\n",
    "\n",
    "A question arises: what is the minimum expected codeword length $L$ that we can achieve? The answer is given by [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) which states that the minimum expected codeword length $L$ is bounded by the entropy $H$ of the source:\n",
    "\n",
    "$$\n",
    "L \\geq H, \\tag{2}\n",
    "$$\n",
    "\n",
    "where the entropy $H$ is defined as:\n",
    "\n",
    "$$\n",
    "H = -\\sum_{i=1}^{n} p_i \\log_2 p_i, \\tag{3}\n",
    "$$\n",
    "\n",
    "This means that no lossless compression scheme can achieve an expected codeword length less than the entropy of the source.\n",
    "\n",
    "By comparing equations (1) and (2) we can see that equality $L = H$ holds when codeword length is equal to $l(c_i) = -\\log_2 p_i$ for all symbols $s_i$. However, this is not always possible because codeword lengths must be integers. Therefore, we can only achieve $L$ that is close to $H$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7bb51",
   "metadata": {},
   "source": [
    "### Huffman Coding\n",
    "\n",
    "[Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding) is a popular algorithm used for lossless data compression. The basic idea is to assign variable-length codes to input characters, with shorter codes assigned to more frequent characters. This results in a prefix-free binary code, meaning no code is a prefix of any other, which allows for efficient encoding and decoding.\n",
    "\n",
    "The algorithm works by building a binary tree where each leaf node represents a symbol and its probability/frequency. The tree is constructed by repeatedly merging the two nodes with the lowest probabilities until only one node remains, which becomes the root of the tree. The code for each symbol is then determined by traversing the tree from the root to the leaf, assigning a '0' for a left branch and a '1' for a right branch.\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "Consider the \"alphabet\" from previous example, where symbol probabilities are\n",
    "\n",
    "```text\n",
    "A: 0.5\n",
    "B: 0.25\n",
    "C: 0.125\n",
    "D: 0.125\n",
    "```\n",
    "\n",
    "Then the binary tree for this alphabet would look like this:\n",
    "\n",
    "```text\n",
    "        *\n",
    "       / \\\n",
    "      /   \\\n",
    "     *     A(0.5)\n",
    "    / \\\n",
    "   /   \\\n",
    "B(0.25) *\n",
    "       / \\\n",
    "      /   \\\n",
    "  C(0.125) D(0.125)\n",
    "```\n",
    "\n",
    "The algorithm assigns codes by traversing from root to leaf: left=0, right=1. So the resulting code for each symbol would be:\n",
    "\"\n",
    "```text\n",
    "A: 1\n",
    "B: 01\n",
    "C: 001\n",
    "D: 000\n",
    "```\n",
    "\n",
    "An average code length $L$ can be calculated as follows:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^4 p_i \\cdot l_i = \\frac{1}{2} \\cdot 1 + \\frac{1}{4} \\cdot 2 + \\frac{1}{8} \\cdot 3 + \\frac{1}{8} \\cdot 3  = 1.75\n",
    "$$\n",
    "\n",
    "For comparison, entropy $H$ can be calculated as follows:\n",
    "\n",
    "$$\n",
    "H = - \\sum_{i=1}^4 p_i \\cdot \\log_2p_i = \\frac{1}{2} \\log_22 + \\frac{1}{4} \\log_24 + \\frac{1}{8} \\log_28 + \\frac{1}{8} \\log_28 = 1.75\n",
    "$$\n",
    "\n",
    "As we can see, the average code length $L$ is equal to the entropy $H$ for this particular example. This is a special case and may not hold for all probability distributions, but it illustrates the relationship between Huffman coding and entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba9acb",
   "metadata": {},
   "source": [
    "### Heap\n",
    "\n",
    "A heap is a specialized tree-based data structure that satisfies the heap property. In a min-heap, for example, the key at a parent node is always less than or equal to the keys of its children, and the smallest key is at the root. This property makes heaps useful for implementing priority queues, where the highest (or lowest) priority element can be efficiently accessed.\n",
    "\n",
    "#### Operation cost\n",
    "\n",
    "The time complexity for the main operations on a heap is as follows:\n",
    "\n",
    "- Insertion: O(log n)\n",
    "- Deletion (removing the root): O(log n)\n",
    "- Accessing the minimum element: O(1)\n",
    "\n",
    "#### Example 3\n",
    "\n",
    "Let's use a min-heap to illustrate the heap property. Consider the following sequence of numbers: 5, 3, 8, 1, 4. We can insert these numbers into a min-heap as follows:\n",
    "```text\n",
    "1. Insert 5:       5\n",
    "2. Insert 3:       3\n",
    "                  /\n",
    "                 5\n",
    "3. Insert 8:       3\n",
    "                  / \\\n",
    "                 5   8\n",
    "4. Insert 1:       1\n",
    "                  / \\\n",
    "                 3   8\n",
    "                /\n",
    "               5\n",
    "5. Insert 4:       1\n",
    "                  / \\\n",
    "                 3   4\n",
    "                / \\\n",
    "               5   8\n",
    "6. Remove 1:       3\n",
    "                  / \\\n",
    "                 5   4\n",
    "                /\n",
    "               8\n",
    "```\n",
    "In this min-heap, the smallest element (1) is at the root, and the heap property is maintained at each level.\n",
    "\n",
    "Let's use `heapq` in Python to demonstrate heap operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00124219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heap: [1, 3, 8, 5, 4]\n",
      "Removed smallest element: 1\n",
      "Heap after removal: [3, 4, 8, 5]\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "# Create a min-heap\n",
    "heap = []\n",
    "\n",
    "# Insert elements into the heap\n",
    "heapq.heappush(heap, 5)\n",
    "heapq.heappush(heap, 3)\n",
    "heapq.heappush(heap, 8)\n",
    "heapq.heappush(heap, 1)\n",
    "heapq.heappush(heap, 4)\n",
    "\n",
    "# Display the heap\n",
    "print(\"Heap:\", heap)\n",
    "\n",
    "# Remove the smallest element\n",
    "smallest = heapq.heappop(heap)\n",
    "print(\"Removed smallest element:\", smallest)\n",
    "print(\"Heap after removal:\", heap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ede9c",
   "metadata": {},
   "source": [
    "### FASTA and FASTQ formats\n",
    "\n",
    "The [FASTA](https://en.wikipedia.org/wiki/FASTA_format) and [FASTQ](https://en.wikipedia.org/wiki/FASTQ_format) file formats are widely used for storing biological sequence data.\n",
    "\n",
    "- The FASTA format is a text-based format for representing nucleotide sequences. Each entry in a FASTA file begins with a header line starting with '>', followed by the sequence identifier. The sequence itself is represented on the following lines. Here's a simple example of a FASTA entry:\n",
    "\n",
    "```text\n",
    ">SEQ_ID\n",
    "GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n",
    "```\n",
    "\n",
    "- The FASTQ format is a text-based format for storing both a biological sequence (usually nucleotide sequences) and its corresponding quality scores. Each entry in a FASTQ file consists of four lines:\n",
    "\n",
    "1. A header line starting with '@' followed by a sequence identifier.\n",
    "2. The raw sequence of nucleotides.\n",
    "3. A separator line starting with '+' (optionally followed by the same sequence identifier).\n",
    "4. A line of quality scores corresponding to the sequence, encoded as ASCII characters.\n",
    "\n",
    "Here's a simple example of a FASTQ entry:\n",
    "\n",
    "```text\n",
    "@SEQ_ID\n",
    "GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT\n",
    "+\n",
    "!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65\n",
    "```\n",
    "\n",
    "In this example, the sequence is `GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT` and the quality scores are represented by the ASCII characters `!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392ac56",
   "metadata": {},
   "source": [
    "# Let's play around"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da32a7",
   "metadata": {},
   "source": [
    "## Huffman coding algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcb871",
   "metadata": {},
   "source": [
    "### Description \n",
    "\n",
    "Huffman coding algorithm follows these steps:\n",
    "\n",
    "1. Build a priority queue (min-heap) of nodes, where each node represents a symbol and its frequency.\n",
    "2. While there is more than one node in the queue:\n",
    "   a. Remove the two nodes of lowest frequency from the queue.\n",
    "   b. Create a new internal node with these two nodes as children and a frequency equal to the sum of their frequencies.\n",
    "   c. Insert the new node back into the queue.\n",
    "3. The remaining node is the root of the Huffman tree.\n",
    "4. Traverse the tree to generate the Huffman codes for each symbol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d87e9f",
   "metadata": {},
   "source": [
    "### Implement a binary-tree class\n",
    "\n",
    "Binary tree is an essential data structure for this algorithm. It is a hierarchical structure where each node has at most two children. In the context of Huffman coding, each leaf node represents a symbol and its probability/frequency, while internal nodes represent the combined frequency of their child nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0220b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    weight: float = 0.0             # weight of the node (frequency or probability)\n",
    "    symbol: Optional[str] = None    # symbol for leaf nodes\n",
    "    left: Optional[\"Node\"] = None   # left child\n",
    "    right: Optional[\"Node\"] = None  # right child"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300c46c",
   "metadata": {},
   "source": [
    "Let's build a huffman tree from example above\n",
    "\n",
    "\n",
    "```text\n",
    "        *\n",
    "       / \\\n",
    "      /   \\\n",
    "     *     A(0.5)\n",
    "    / \\\n",
    "   /   \\\n",
    "B(0.25) *\n",
    "       / \\\n",
    "      /   \\\n",
    "  C(0.125) D(0.125)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aec2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree structure:\n",
      "└── * (1.0)\n",
      "    ├── * (0.5)\n",
      "    │   ├── B (0.25)\n",
      "    │   └── * (0.25)\n",
      "    │       ├── C (0.125)\n",
      "    │       └── D (0.125)\n",
      "    └── A (0.5)\n"
     ]
    }
   ],
   "source": [
    "# Create leaf nodes\n",
    "node_A = Node(weight=0.5, symbol=\"A\")\n",
    "node_B = Node(weight=0.25, symbol=\"B\") \n",
    "node_C = Node(weight=0.125, symbol=\"C\")\n",
    "node_D = Node(weight=0.125, symbol=\"D\")\n",
    "\n",
    "# Create internal nodes (bottom-up)\n",
    "# Right internal node: combines C and D\n",
    "right_internal = Node(weight=0.25, left=node_C, right=node_D)\n",
    "\n",
    "# Left internal node: combines B and the right internal node  \n",
    "left_internal = Node(weight=0.5, left=node_B, right=right_internal)\n",
    "\n",
    "# Root node: combines left internal node and A\n",
    "root = Node(weight=1.0, left=left_internal, right=node_A)\n",
    "\n",
    "# Visualise\n",
    "print(\"Tree structure:\")\n",
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d064f900",
   "metadata": {},
   "source": [
    "### Build a Huffman Tree\n",
    "\n",
    "To efficiently manage the nodes, we'll use a priority queue (min-heap) to store and retrieve the nodes based on their frequencies. This allows us to quickly access the least frequent nodes when building the tree. The algorithm works by repeatedly merging the two least frequent nodes until only one node remains, which becomes the root of the Huffman tree. Schematic representation of the process:\n",
    "\n",
    "#### Example 4\n",
    "\n",
    "Let's use the following symbols and their frequencies:\n",
    "\n",
    "- A: 0.5\n",
    "- B: 0.25\n",
    "- C: 0.125\n",
    "- D: 0.125\n",
    "\n",
    "1. Create all leaf nodes for our future tree\n",
    "    ```python\n",
    "    Node(A, 0.5)\n",
    "    Node(B, 0.25)\n",
    "    Node(C, 0.125)\n",
    "    Node(D, 0.125)\n",
    "    ```\n",
    "2. Create a priority queue (min-heap) and insert all leaf nodes:\n",
    "    ```python\n",
    "    min_heap = [Node(A, 0.5), Node(B, 0.25), Node(C, 0.125), Node(D, 0.125)]\n",
    "    ```\n",
    "\n",
    "3. Extract the two least frequent nodes:\n",
    "    ```python\n",
    "    left = extract_min(min_heap) # Node(C, 0.125)\n",
    "    right = extract_min(min_heap) # Node(D, 0.125)\n",
    "    ```\n",
    "\n",
    "4. Create a new internal node with these two nodes as children and their combined frequency:\n",
    "    ```python\n",
    "    new_node = Node(None, left.frequency + right.frequency, left, right)\n",
    "    ```\n",
    "\n",
    "5. Insert the new node back into the priority queue:\n",
    "    ```python\n",
    "    insert(min_heap, new_node) # [Node(B, 0.25), Node(A, 0.5), Node(None, 0.25, Node(C, 0.125), Node(D, 0.125))]\n",
    "    ```\n",
    "\n",
    "6. Repeat steps 3-5 until only one node remains in the priority queue. This node will be the root of the Huffman tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e24a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_huffman_tree(weights: Dict[str, float]) -> Optional[Node]:\n",
    "    \"\"\"\n",
    "    Build a Huffman tree from the given symbol weights\n",
    "    \"\"\"\n",
    "    # Check that weights are > 0\n",
    "    items = [(float(w), str(s)) for s, w in weights.items() if float(w) > 0.0]\n",
    "    if not items:\n",
    "        return None\n",
    "\n",
    "    # Sort items by weight to make code deterministic (important for decoding)\n",
    "    items.sort(key=lambda x: (x[0], x[1]))\n",
    "\n",
    "    # Build the tree using a min-heap\n",
    "    uid = count()                                   # unique integer 0,1,2,...\n",
    "    heap = [(w, next(uid), Node(weight=w, symbol=s))   # (weight, tie_id, tree)\n",
    "            for (w, s) in items]\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    # Handle single-symbol edge case\n",
    "    if len(heap) == 1:\n",
    "        w, _, leaf = heapq.heappop(heap)\n",
    "        return Node(weight=w, left=leaf)\n",
    "\n",
    "    # Build the tree\n",
    "    while len(heap) > 1:\n",
    "        w1, _, n1 = heapq.heappop(heap)             # smallest\n",
    "        w2, _, n2 = heapq.heappop(heap)             # 2nd smallest\n",
    "        parent = Node(weight=w1 + w2, left=n1, right=n2)\n",
    "        heapq.heappush(heap, (parent.weight, next(uid), parent))\n",
    "\n",
    "    return heap[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4422b9",
   "metadata": {},
   "source": [
    "Now we can apply the algorithm to build the Huffman tree and generate the codes for each symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f0fcebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huffman Tree structure:\n",
      "└── * (1.0)\n",
      "    ├── A (0.5)\n",
      "    └── * (0.5)\n",
      "        ├── B (0.25)\n",
      "        └── * (0.25)\n",
      "            ├── C (0.125)\n",
      "            └── D (0.125)\n"
     ]
    }
   ],
   "source": [
    "probabilities = {\n",
    "    'A': 0.5,\n",
    "    'B': 0.25,\n",
    "    'C': 0.125,\n",
    "    'D': 0.125\n",
    "}\n",
    "\n",
    "root = build_huffman_tree(probabilities)\n",
    "print(\"Huffman Tree structure:\")\n",
    "print_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75de47",
   "metadata": {},
   "source": [
    "### Derive codes\n",
    "\n",
    "To derive codes from Huffman tree we will use a recursive traversal of the tree. Starting from the root, we will traverse to the left child by appending '0' to the current code and to the right child by appending '1'. When we reach a leaf node, we will record the symbol and its corresponding code. This is essentially a depth-first search (DFS) of the tree.\n",
    "\n",
    "Schematic representation of the process:\n",
    "\n",
    "1. Start at the root with an empty code:\n",
    "   ```python\n",
    "   current_code = \"\"\n",
    "   ```\n",
    "\n",
    "2. Traverse the tree:\n",
    "   - If the current node is a leaf, record the symbol and its code:\n",
    "     ```python\n",
    "     if node is leaf:\n",
    "         codes[node.symbol] = current_code\n",
    "     ```\n",
    "\n",
    "   - If the current node is not a leaf, traverse left and right:\n",
    "     ```python\n",
    "     traverse(node.left, current_code + \"0\")\n",
    "     traverse(node.right, current_code + \"1\")\n",
    "     ```\n",
    "\n",
    "The final codes dictionary will contain the Huffman codes for all symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0494c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes_from_tree(root: Optional[Node]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Derive binary codes from the Huffman tree\n",
    "    \"\"\"\n",
    "    # Return empty dictionary if root is None\n",
    "    if root is None:\n",
    "        return {}\n",
    "\n",
    "    # Create a dictionary to hold the codes\n",
    "    codes: Dict[str, str] = {}\n",
    "\n",
    "    # Depth-first search (DFS) to traverse the tree\n",
    "    def dfs(n: Node, path: str):\n",
    "        if n.symbol is not None:                 # leaf node found\n",
    "            codes[n.symbol] = path or \"0\"        # assign code\n",
    "            return\n",
    "        if n.left:  dfs(n.left,  path + \"0\") # go left, add \"0\"\n",
    "        if n.right: dfs(n.right, path + \"1\") # go right, add \"1\"\n",
    "\n",
    "    # Start DFS traversal from the root\n",
    "    dfs(root, \"\")\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9978bb",
   "metadata": {},
   "source": [
    "Now we can apply this algorithm to generate the Huffman codes for each symbol from the constructed Huffman tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e2a3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huffman Codes: {'A': '0', 'B': '10', 'C': '110', 'D': '111'}\n"
     ]
    }
   ],
   "source": [
    "codes = codes_from_tree(root)\n",
    "print(\"Huffman Codes:\", codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9fe0f",
   "metadata": {},
   "source": [
    "As you can see the code is very similar to the one we got in [Exercise 2](#example-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b8c2d",
   "metadata": {},
   "source": [
    "### Compare entropy and expected length\n",
    "\n",
    "Although we have already compared entropy and expected length in Exercise 2. Let's create functions to compute both values for future examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3add998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities: Dict[str, float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a set of symbol probabilities\n",
    "    \"\"\"\n",
    "    p = np.array(list(probabilities.values()))\n",
    "    return -np.sum(p * np.log2(p, where=(p > 0)))\n",
    "\n",
    "def expected_length(coding: Dict[str, str], probabilities: Dict[str, float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the expected length of a set of symbol codes\n",
    "    \"\"\"\n",
    "    keys = coding.keys()\n",
    "    p = np.array([probabilities.get(symbol, 0) for symbol in keys])\n",
    "    length = np.array([len(coding[symbol]) for symbol in keys])\n",
    "    return np.sum(length * p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55ed10",
   "metadata": {},
   "source": [
    "Now let's print the summary for our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6206f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol Probabilities: {'A': 0.5, 'B': 0.25, 'C': 0.125, 'D': 0.125}\n",
      "Huffman Codes: {'A': '0', 'B': '10', 'C': '110', 'D': '111'}\n",
      "Huffman tree from weights:\n",
      "└── * (1.0)\n",
      "    ├── A (0.5)\n",
      "    └── * (0.5)\n",
      "        ├── B (0.25)\n",
      "        └── * (0.25)\n",
      "            ├── C (0.125)\n",
      "            └── D (0.125)\n",
      "Entropy: 1.75\n",
      "Expected Length: 1.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Symbol Probabilities:\", probabilities)\n",
    "print(\"Huffman Codes:\", codes)\n",
    "print(\"Huffman tree from weights:\")\n",
    "print_tree(root)\n",
    "print(\"Entropy:\", entropy(probabilities))\n",
    "print(\"Expected Length:\", expected_length(codes, probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a361fa2",
   "metadata": {},
   "source": [
    "### Convert a string of charachters to bytes\n",
    "\n",
    "Let's create a function to convert a string to bytes and compare the size of our byte representation to **ASCII**, **UTF-8** and naive encodings. Both **ASCII** and **UTF-8** are used in [FASTQ](https://en.wikipedia.org/wiki/FASTQ_format) format to store sequencing information. By naive encoding I mean the following list of codewords:\n",
    "\n",
    "```text\n",
    "A: 00, B: 01, C: 10, D: 11\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68dc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits2bytes(bits: str):\n",
    "    \"\"\"\n",
    "    Convert a string of bits to bytes\n",
    "    \"\"\"\n",
    "    # Pad the bit string to make its length a multiple of 8\n",
    "    paddlen = (8 - len(bits) % 8) % 8\n",
    "    padded_bits = bits + '0' * paddlen\n",
    "\n",
    "    # Convert each group of 8 bits to a byte\n",
    "    byte_array = bytearray()\n",
    "    for i in range(0, len(padded_bits), 8):\n",
    "        byte = padded_bits[i:i + 8]\n",
    "        byte_array.append(int(byte, 2))\n",
    "    return bytes(byte_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6a718",
   "metadata": {},
   "source": [
    "Now let's generate a string of characters from our distribution and generate a bit and byte string representations:\n",
    "\n",
    "```text\n",
    "A: 0.5, B: 0.25, C: 0.125, D: 0.125\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4367ad83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated string: DBDBBADAAACACDABAAADADCCAABABABDBAABBBABABCAABBABB...\n",
      "Character counts: {'A': 50057, 'B': 24819, 'C': 12415, 'D': 12709}\n",
      "Character frequencies: {'A': 0.50057, 'B': 0.24819, 'C': 0.12415, 'D': 0.12709}\n"
     ]
    }
   ],
   "source": [
    "# fix random seed\n",
    "np.random.seed(4)\n",
    "\n",
    "# generate string\n",
    "n = 100000\n",
    "char_list = list(probabilities.keys())\n",
    "prob_list = list(probabilities.values())\n",
    "string = \"\".join(np.random.choice(char_list, p=prob_list, size=n))\n",
    "print(f\"Generated string: {string[:50]}...\")\n",
    "\n",
    "# calculate frequency\n",
    "freq = {char: string.count(char) for char in char_list}\n",
    "print(\"Character counts:\", freq)\n",
    "print(\"Character frequencies:\", {char: freq[char] / n for char in char_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911f6196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huffman encoding size: 21884 bytes (175067 bits)\n",
      "Naive encoding size: 25000 bytes (200000 bits), it's  1.142 times larger than Huffman encoding\n",
      "UTF-8 encoding size: 100000 bytes, it's  4.570 times larger than Huffman encoding\n",
      "ASCII encoding size: 100000 bytes, it's  4.570 times larger than Huffman encoding\n"
     ]
    }
   ],
   "source": [
    "# encode the string using different encoding schemes\n",
    "huffman_bits = \"\".join([codes.get(char) for char in string])\n",
    "huffman_bytes = bits2bytes(huffman_bits)\n",
    "naive_codes = {\"A\": \"00\", \"B\": \"01\", \"C\": \"10\", \"D\": \"11\"}\n",
    "naive_bits = \"\".join([naive_codes.get(char) for char in string])\n",
    "naive_bytes = bits2bytes(naive_bits)\n",
    "utf8_bytes = string.encode('utf-8')\n",
    "ascii_bytes = string.encode('ascii')\n",
    "\n",
    "# print sizes\n",
    "print(f\"Huffman encoding size: {len(huffman_bytes)} bytes ({len(huffman_bits)} bits)\")\n",
    "print(f\"Naive encoding size: {len(naive_bytes)} bytes ({len(naive_bits)} bits), it's {len(naive_bits) / len(huffman_bits): .3f} times larger than Huffman encoding\")\n",
    "#print(f\"Naive encoding size (bytes): {len(naive_bytes)}, it's {len(naive_bytes) / len(huffman_bytes): .3f} times larger than Huffman encoding\")\n",
    "print(f\"UTF-8 encoding size: {len(utf8_bytes)} bytes, it's {len(utf8_bytes) / len(huffman_bytes): .3f} times larger than Huffman encoding\")\n",
    "print(f\"ASCII encoding size: {len(ascii_bytes)} bytes, it's {len(ascii_bytes) / len(huffman_bytes): .3f} times larger than Huffman encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146fc375",
   "metadata": {},
   "source": [
    "These results are not very surprising. Huffman encoding is exactly $\\frac{L_{naive}}{L_{huffman}} = \\frac{2}{1.75} \\approx 1.142$ times shorter than naive encoding. Moreover UTF-8 and ASCII encodings use 8 bits per character, so they are $\\frac{8}{1.75} \\approx 4.571$ times longer than Huffman encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf847d",
   "metadata": {},
   "source": [
    "### File header\n",
    "\n",
    "The \"header\" is everything you need to prepend so another program can rebuild the codebook and decode the data. In our case it will be a mapping of symbols to their corresponding Huffman codes. The \"cannonical Huffman\" idea is to use code lengths to rebuild the codebook without needing to transmit the actual codes. This is possible because in canonical Huffman, codes of the same length are consecutive integers.\n",
    "\n",
    "The canonical assignment rule:\n",
    "\n",
    "1. Collect and sort symbols by code length and lexicographic order (for tie-breaking)\n",
    "2. Count how many codes of each length\n",
    "3. Assign codes in the following manner:\n",
    "    - Skip past all codes assigned at length $l - 1$ (we skip already taken prefixes basically)\n",
    "    - Assign the next available codes for length $l$\n",
    "\n",
    "#### Example 5\n",
    "\n",
    "Let's say I have a following list of code lengths: 1 code of length 1, 1 code of length 2 and 2 codes of length 3:\n",
    "\n",
    "```text\n",
    "                            {1: 1, 2: 1, 3: 2}\n",
    "```\n",
    "\n",
    "\n",
    "Let's assign codes step by step:\n",
    "1. For length 1, we have 1 code. The smallest available code is 0, so we assign it. This way we block all codes starting with 0 (as we a uniquely decodable codes).\n",
    "    ```python\n",
    "    codebook[0] = 0\n",
    "    last_code = 0\n",
    "    ```\n",
    "2. To get a next available prefix we take binary representation of `last_code + 1`, which is `1` in this case. We then make a [binary shift](https://python-central-hub.vercel.app/tutorials/python-operator/bitwise-operators/#bitwise-left-shift) (pad it with zeros to the right) to get a first code of length 2: `10`. Finally we assign all codes of length 2 (in our case only one):\n",
    "    ```python\n",
    "    last_code = last_code + 1 # 1\n",
    "    first_code_length_2 = last_code << 1 # 2 which is equal 10 in binary\n",
    "    codebook[1] = first_code_length_2\n",
    "    last_code = codebook[1]\n",
    "    ```\n",
    "3. To assign codes of length 3, we repeat the process. The next available prefix is `last_code + 1`, which is `3`. We pad it to get the first code of length 3: `110`. We assign all codes of length 3 (in our case two codes):\n",
    "    ```python\n",
    "    last_code = last_code + 1 # 3\n",
    "    first_code_length_3 = last_code << 1 # 6 which is equal 110 in binary\n",
    "    codebook[2] = first_code_length_3\n",
    "    codebook[3] = first_code_length_3 + 1 # 7 which is equal 111 in binary\n",
    "    ```\n",
    "4. Finaly let's convert integers to binary strings:\n",
    "    ```python\n",
    "    codebook = [bin(code)[2:] for code in codebook]\n",
    "    # codebook = ['0', '10', '110', '111']\n",
    "    ```\n",
    "\n",
    "This can be illustrated on the table of all possible codes:\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "\n",
    "    .highlight-blue {\n",
    "        background-color: #007bff !important;\n",
    "        color: white !important;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>length 1</th>\n",
    "      <th>length 2</th>\n",
    "      <th>length 3</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th rowspan=\"4\" valign=\"top\" class=\"highlight-blue\">0 = 0</th>\n",
    "      <th rowspan=\"2\" valign=\"top\">00 = 0</th>\n",
    "      <th>000 = 0</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>001 = 1</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"2\" valign=\"top\">01 = 1</th>\n",
    "      <th>010 = 2</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>011 = 3</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"4\" valign=\"top\">1 = 1</th>\n",
    "      <th rowspan=\"2\" valign=\"top\" class=\"highlight-blue\">10 = 2</th>\n",
    "      <th>100 = 4</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>101 = 5</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"2\" valign=\"top\">11 = 3</th>\n",
    "      <th class=\"highlight-blue\">110 = 6</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th class=\"highlight-blue\">111 = 7</th>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "In the same way we could do for the following list of code lengths:\n",
    "\n",
    "```text\n",
    "                                {1: 0, 2: 2, 3: 3}\n",
    "```\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "\n",
    "    .highlight-blue {\n",
    "        background-color: #007bff !important;\n",
    "        color: white !important;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>length 1</th>\n",
    "      <th>length 2</th>\n",
    "      <th>length 3</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th rowspan=\"4\" valign=\"top\">0 = 0</th>\n",
    "      <th rowspan=\"2\" valign=\"top\" class=\"highlight-blue\">00 = 0</th>\n",
    "      <th>000 = 0</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>001 = 1</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"2\" valign=\"top\" class=\"highlight-blue\">01 = 1</th>\n",
    "      <th>010 = 2</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>011 = 3</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"4\" valign=\"top\">1 = 1</th>\n",
    "      <th rowspan=\"2\" valign=\"top\">10 = 2</th>\n",
    "      <th class=\"highlight-blue\">100 = 4</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th class=\"highlight-blue\">101 = 5</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"2\" valign=\"top\">11 = 3</th>\n",
    "      <th class=\"highlight-blue\" class=\"highlight-blue\">110 = 6</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>111 = 7</th>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd29da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonical_codes_from_lengths(codelens: Dict[str, str]) -> Dict[str, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Generate canonical Huffman codes from code lengths\n",
    "    \"\"\"\n",
    "    # Filter out zeros\n",
    "    items = [(L, ord(s), s) for s, L in codelens.items() if L > 0]\n",
    "    if not items:\n",
    "        return {}\n",
    "    \n",
    "    # Sort by (length, codepoint) for determinism\n",
    "    items.sort(key=lambda x: (x[0], x[1]))\n",
    "    \n",
    "\n",
    "    # Count how many codes of each length\n",
    "    max_len = max(L for L, _, _ in items)\n",
    "    bl_count = [0] * (max_len + 1)\n",
    "    for L, _, _ in items:\n",
    "        bl_count[L] += 1\n",
    "    \n",
    "    # Compute first code for each length\n",
    "    next_code = [0] * (max_len + 1)\n",
    "    code = 0\n",
    "    for L in range(1, max_len + 1):\n",
    "        code = (code + (bl_count[L - 1] if L - 1 >= 0 else 0)) * 2\n",
    "        next_code[L] = code\n",
    "\n",
    "    # Assign codes in (length, symbol) order\n",
    "    out: Dict[str, Tuple[int, int]] = {}\n",
    "    for L, _, s in items:\n",
    "        out[s] = (next_code[L], L)\n",
    "        next_code[L] += 1\n",
    "    return out\n",
    "\n",
    "def format_bits(code: int, L: int) -> str:\n",
    "    return format(code, f\"0{L}b\") if L > 0 else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7880a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code lengths: {'A': 1, 'B': 2, 'C': 3, 'D': 3}\n",
      "Original codes: {'A': '0', 'B': '10', 'C': '110', 'D': '111'}\n",
      "Canonical codes: {'A': '0', 'B': '10', 'C': '110', 'D': '111'}\n"
     ]
    }
   ],
   "source": [
    "code_lengths = {symbol: len(code) for symbol, code in codes.items()}\n",
    "print(\"Code lengths:\", code_lengths)\n",
    "\n",
    "canonical_codes = canonical_codes_from_lengths(code_lengths)\n",
    "formatted_bits = {symbol: format_bits(code, L) for symbol, (code, L) in canonical_codes.items()}\n",
    "print(\"Original codes:\", codes)\n",
    "print(\"Canonical codes:\", formatted_bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc92c3",
   "metadata": {},
   "source": [
    "Now we can construct the header:\n",
    "\n",
    "```text\n",
    "File format (2 bytes): 'HC'\n",
    "Alphabet size (2 bytes): N\n",
    "Code lengths (4 bytes per symbol + 1 byte for length): {symbol: length}\n",
    "Bit length (4 bytes): L\n",
    "```\n",
    "\n",
    "This header is self-describing. Given that you know the file format is 'HC', you can easily parse the rest of the header fields:\n",
    "1. Read the alphabet size (2 bytes).\n",
    "2. For each symbol, read its code length (4 bytes + 1 byte).\n",
    "3. Finally, read the total bit length (4 bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53b726bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_header(code_lengths: Dict[str, int], bitlen: int) -> bytes:\n",
    "    \"\"\"\n",
    "    Write the header for the compressed file:\n",
    "        MAGIC(2)='HC' | N:uint16 | N×(codepoint:uint32, codelen:uint8) | BITLEN:uint32\n",
    "    \"\"\"\n",
    "    # Get code lengths and sort them by (length, codepoint)\n",
    "    entries = [(ord(s), int(L)) for s, L in code_lengths.items() if L > 0]\n",
    "    entries.sort(key=lambda x: (x[1], x[0]))\n",
    "    N = len(entries)\n",
    "\n",
    "    # Write the header\n",
    "    parts = []\n",
    "\n",
    "    parts.append(struct.pack(\">2sH\", b\"HC\", N)) # Magic number and entry count: 2 bytes + 2 bytes\n",
    "    for codepoint, codelen in entries:\n",
    "        parts.append(struct.pack(\">IB\", codepoint, codelen)) # Codepoint and code length: uint32 + uint8\n",
    "    parts.append(struct.pack(\">I\", bitlen)) # Total bit length: uint32\n",
    "    return b\"\".join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e90f10",
   "metadata": {},
   "source": [
    "Let's generate the header for the string that we generated earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d525fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header size: 28 bytes\n",
      "Header in hex: 4843000400000041010000004202000000430300000044030002abdb\n"
     ]
    }
   ],
   "source": [
    "header = write_header(code_lengths, len(huffman_bits))\n",
    "print(\"Header size:\", len(header), \"bytes\")\n",
    "print(\"Header in hex:\", header.hex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed12987",
   "metadata": {},
   "source": [
    "Let's also create a function to decode the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18e28818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_header(data: bytes, offset: int = 0):\n",
    "    \"\"\"\n",
    "    Parse the header and return the code lengths, bitlen and offset\n",
    "    Args:\n",
    "        data (bytes): The header data\n",
    "        offset (int, optional): The starting bit offset. Defaults to 0.\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], int, int]: The code lengths, bitlen and offset\n",
    "    \"\"\"\n",
    "    # Read the filetype\n",
    "    magic, = struct.unpack_from(\">2s\", data, offset); offset += 2\n",
    "\n",
    "    if magic != b\"HC\":\n",
    "        raise ValueError(\"bad magic: expected b'HC'\")\n",
    "\n",
    "    # Read number of symbols\n",
    "    (N,) = struct.unpack_from(\">H\", data, offset); offset += 2\n",
    "\n",
    "    # Read code lengths\n",
    "    code_lengths: Dict[str, int] = {}\n",
    "    for _ in range(N):\n",
    "        codepoint, codelen = struct.unpack_from(\">IB\", data, offset); offset += 5\n",
    "        code_lengths[chr(codepoint)] = codelen\n",
    "\n",
    "    # Read bitlen\n",
    "    (bitlen,) = struct.unpack_from(\">I\", data, offset); offset += 4\n",
    "    return code_lengths, bitlen, offset # offset now points to the start of PAYLOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa6e4f",
   "metadata": {},
   "source": [
    "We can try to decode our header now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50ae6c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original code lengths: {'A': 1, 'B': 2, 'C': 3, 'D': 3}\n",
      "Decoded code lengths: {'A': 1, 'B': 2, 'C': 3, 'D': 3}\n",
      "Bit length: 175067\n",
      "Offset: 28\n"
     ]
    }
   ],
   "source": [
    "decoded_code_lengths, bitlen, offset = read_header(header)\n",
    "print(\"Original code lengths:\", code_lengths)\n",
    "print(\"Decoded code lengths:\", decoded_code_lengths)\n",
    "print(\"Bit length:\", bitlen)\n",
    "print(\"Offset:\", offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92143f",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "At this point we now how to:\n",
    "1. Create code words from probabilities using Huffman coding algorithm\n",
    "2. Encode the data using the generated code words\n",
    "3. Create a header that contains all necessary information to decode the data:\n",
    "   - Original code lengths\n",
    "   - Decoded code lengths\n",
    "   - Bit length\n",
    "   - Offset\n",
    "4. Reconstruct code words from the encoded data using the header information\n",
    "\n",
    "The final step is to decode the data using the reconstructed code words and the header information. A Huffman code is a **prefix-free**, so you can read the bitstream one bit at a time and walk a binary tree: `0` = go left, `1` = go right. Whenever you land on a leaf, you’ve finished exactly one symbol. Emit it and jump back to the root to start the next symbol.\n",
    "\n",
    "But before doing that we first need to construct the Huffman tree from the code lengths. Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9245979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tree(canon: Dict[str, Tuple[int, int]]) -> Node:\n",
    "    \"\"\"\n",
    "    Decode a Huffman tree from canonical code words\n",
    "    \"\"\"\n",
    "    root = Node()\n",
    "    for symbol, (code, length) in canon.items():\n",
    "        current = root\n",
    "        # Walk from root following the binary path\n",
    "        for i in range(length - 1, -1, -1):  # MSB to LSB\n",
    "            bit = (code >> i) & 1            # Extract bit i\n",
    "            if bit == 0:\n",
    "                # Go left, create node if needed\n",
    "                if current.left is None:\n",
    "                    current.left = Node(weight=0)\n",
    "                current = current.left\n",
    "            else:\n",
    "                # Go right, create node if needed  \n",
    "                if current.right is None:\n",
    "                    current.right = Node(weight=0)\n",
    "                current = current.right\n",
    "        # Mark final node as leaf with this symbol\n",
    "        current.symbol = symbol\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69090432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code lengths: {'A': 1, 'B': 2, 'C': 3, 'D': 3}\n",
      "Original codes: {'A': '0', 'B': '10', 'C': '110', 'D': '111'}\n",
      "Canonical codes: {'A': '0', 'B': '10', 'C': '110', 'D': '111'}\n",
      "Decoding Tree structure:\n",
      "└── * (N/A)\n",
      "    ├── A (0)\n",
      "    └── * (N/A)\n",
      "        ├── B (0)\n",
      "        └── * (N/A)\n",
      "            ├── C (0)\n",
      "            └── D (0)\n"
     ]
    }
   ],
   "source": [
    "code_lengths = {symbol: len(code) for symbol, code in codes.items()}\n",
    "print(\"Code lengths:\", code_lengths)\n",
    "\n",
    "canonical_codes = canonical_codes_from_lengths(code_lengths)\n",
    "formatted_bits = {symbol: format_bits(code, L) for symbol, (code, L) in canonical_codes.items()}\n",
    "print(\"Original codes:\", codes)\n",
    "print(\"Canonical codes:\", formatted_bits)\n",
    "decode_tree = decode_tree(canonical_codes)\n",
    "print(\"Decoding Tree structure:\")\n",
    "print_tree(decode_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fe55c",
   "metadata": {},
   "source": [
    "All we need to do now to decode a stream of bits is to traverse the decoding tree according to the bit values. Starting from the root, we move left for a '0' and right for a '1', until we reach a leaf node, which gives us the decoded symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1aadc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_huffman(encoded_str: str, root: Node) -> str:\n",
    "    \"\"\"\n",
    "    Decode a Huffman encoded string\n",
    "    \"\"\"\n",
    "    decoded_output = []\n",
    "    current_node = root\n",
    "\n",
    "    for bit in encoded_str:\n",
    "        # Traverse the tree\n",
    "        current_node = current_node.left if bit == '0' else current_node.right\n",
    "\n",
    "        # Raise an error if we reach an invalid node\n",
    "        if current_node is None:\n",
    "            raise ValueError(\"Invalid encoded string\")\n",
    "\n",
    "        # If we reach a leaf node, append the symbol\n",
    "        if current_node and current_node.symbol:\n",
    "            decoded_output.append(current_node.symbol)\n",
    "            current_node = root  # Reset for the next symbol\n",
    "\n",
    "    return ''.join(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a0d74",
   "metadata": {},
   "source": [
    "Finally let's decode an encoded string using our Huffman tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34208da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: DBDBBADAAACACDABAAADADCCAABABA...ABBAADBDAADABABABAAAADCADADAAD\n",
      "Decoded string:  DBDBBADAAACACDABAAADADCCAABABA...ABBAADBDAADABABABAAAADCADADAAD\n"
     ]
    }
   ],
   "source": [
    "decoded_string = decode_huffman(huffman_bits, decode_tree)\n",
    "\n",
    "# Check if the strings are the same\n",
    "assert string == decoded_string, \"Decoded string does not match the original!\"\n",
    "\n",
    "print(\"Original string:\", string[:30] + \"...\" + string[-30:])\n",
    "print(\"Decoded string: \", decoded_string[:30] + \"...\" + decoded_string[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74f1fd",
   "metadata": {},
   "source": [
    "## Encode human genome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a832e",
   "metadata": {},
   "source": [
    "### Download human genome data\n",
    "\n",
    "The [human genome](https://en.wikipedia.org/wiki/Human_genome) is the complete set of nucleic acid sequences for humans, encoded as DNA within the 23 chromosome pairs in cell nuclei and in a small DNA molecule found within individual mitochondria. The human genome is estimated to have about 3 billion base pairs of DNA, which are divided into approximately 20,000-25,000 genes.\n",
    "\n",
    "The human genome sequence is stored in several formats, including FASTA and FASTQ. The FASTA format is a text-based format for representing nucleotide sequences, while the FASTQ format is used for storing both nucleotide sequences and their corresponding quality scores. Let's download only 21st chromosome from human genome (as it is one of the smallest) from [Ensembl](https://www.ensembl.org/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2d9fdbb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-31 18:51:36--  https://ftp.ensembl.org/pub/release-114/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.21.fa.gz\n",
      "Resolving ftp.ensembl.org (ftp.ensembl.org)... 193.62.193.169\n",
      "Connecting to ftp.ensembl.org (ftp.ensembl.org)|193.62.193.169|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11787503 (11M) [application/x-gzip]\n",
      "Saving to: ‘genome.fa.gz’\n",
      "\n",
      "genome.fa.gz        100%[===================>]  11.24M  12.2MB/s    in 0.9s    \n",
      "\n",
      "2025-08-31 18:51:37 (12.2 MB/s) - ‘genome.fa.gz’ saved [11787503/11787503]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O genome.fa.gz https://ftp.ensembl.org/pub/release-114/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.21.fa.gz\n",
    "!gunzip -f genome.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f29008",
   "metadata": {},
   "source": [
    "### File structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6b9eb",
   "metadata": {},
   "source": [
    "Let's take a look at the downloaded FASTA file for the 21st chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2a3f135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of the file:\n",
      ">21 dna:chromosome chromosome:GRCh38:21:1:46709983:1 REF\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "\n",
      "Middle of the file:\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNGATCATACT\n",
      "TGTAGCATTAGGCCCAGGGATGAGAGTCAACATCATTACAATTAACTATGTCAGGATAGG\n",
      "AGACTCATCCCTTGCCTATGAGCTGAGTTTAGATGTGGGCCACCATTTTAACTCTGGTTG\n",
      "AATGTTTATATATGAACACAGGCCTAGCACCAATGTGATGTGAGTCTTTGGCCTAGACAC\n",
      "TTCAAGCAGGAGGCAATGTGACATATCTCTGGGTCTATCAACTATTTGATATGACCTTCC\n",
      "TTTTTTACCTGAGCTTTCCCCATAAAAGAGATGTGACATATGTCTAGACCCAGCACCTGG\n",
      "GTGATGTGGCTCTTCTTTATTGACTGAGCCCTGTGTATTTTGGGTATTCTGACATATCCC\n",
      "\n",
      "File tail:\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n"
     ]
    }
   ],
   "source": [
    "# File head\n",
    "!echo -e \"Head of the file:\"\n",
    "!head -n 5 genome.fa\n",
    "\n",
    "# Middle of the file\n",
    "!echo -e \"\\nMiddle of the file:\"\n",
    "!sed -n '103524,103530p' genome.fa\n",
    "\n",
    "# File tail\n",
    "!echo -e \"\\nFile tail:\"\n",
    "!tail -n 5 genome.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca43887",
   "metadata": {},
   "source": [
    "The header line in the `genome.fa` file contains the sequence identifier and description, while the subsequent lines contain the nucleotide sequence in FASTA format. Another important aspect is the presence of gaps (represented by 'N's) in the sequence, which indicate regions that are difficult to sequence or assemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecbd5e",
   "metadata": {},
   "source": [
    "### Calculate frequency of each nucleotide\n",
    "\n",
    "To compress this file I will discard the header and focus on the nucleotide sequences. I will also ignore new line characters as they appear every 60 bases and so are easy to recover. We need to count the frequency of each nucleotide (A, T, G, C, N) in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb76aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genome sequence: NNNNNNNNNNNNNNN...TTCTCTGCACTCCAGCCTGG...NNNNNNNNNNNNNNN\n",
      "Chromosome length: 46709983\n"
     ]
    }
   ],
   "source": [
    "# Read genome file\n",
    "with open(\"genome.fa\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    header = lines[0].strip()  # First line is the header\n",
    "    genome = \"\".join(line.strip() for line in lines[1:])  # Join all subsequent lines\n",
    "\n",
    "print(\"Genome sequence:\", genome[:15] + \"...\" + genome[6240000:6240020] + \"...\" + genome[-15:])\n",
    "print(\"Chromosome length:\", len(genome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a61ed789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nucleotide counts/frequencies:\n",
      "A: 11820664 / 0.253\n",
      "T: 11856330 / 0.254\n",
      "G: 8226381 / 0.176\n",
      "C: 8185244 / 0.175\n",
      "N: 6621364 / 0.142\n"
     ]
    }
   ],
   "source": [
    "# Count nucleotide frequencies\n",
    "N = len(genome)\n",
    "counts = Counter(genome)\n",
    "nucleotide_counts = {b: counts.get(b, 0) for b in \"ATGCN\"}\n",
    "nucleotide_frequencies = {k: v / N for k, v in nucleotide_counts.items()}\n",
    "\n",
    "print(\"Nucleotide counts/frequencies:\")\n",
    "for nucleotide, number in nucleotide_counts.items():\n",
    "    print(f\"{nucleotide}: {number} /{number / N: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da8f84",
   "metadata": {},
   "source": [
    "Let's calculate the entropy of the nucleotide frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edef51bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of nucleotide frequencies:  2.285\n"
     ]
    }
   ],
   "source": [
    "entropy_value = entropy(nucleotide_frequencies)\n",
    "print(f\"Entropy of nucleotide frequencies: {entropy_value: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ae95d",
   "metadata": {},
   "source": [
    "### Huffman codes for nucleotides\n",
    "\n",
    "Based on the nucleotide frequencies, we can construct a Huffman tree and generate Huffman codes for each nucleotide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c783e2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol Probabilities: {'A': 0.25306504607376973, 'T': 0.2538286087580036, 'G': 0.17611612061601478, 'C': 0.17523543093560964, 'N': 0.14175479361660226}\n",
      "Huffman Codes: {'G': '00', 'A': '01', 'T': '10', 'N': '110', 'C': '111'}\n",
      "Huffman tree from weights:\n",
      "└── * (1.0)\n",
      "    ├── * (0.4291811666897845)\n",
      "    │   ├── G (0.17611612061601478)\n",
      "    │   └── A (0.25306504607376973)\n",
      "    └── * (0.5708188333102155)\n",
      "        ├── T (0.2538286087580036)\n",
      "        └── * (0.31699022455221193)\n",
      "            ├── N (0.14175479361660226)\n",
      "            └── C (0.17523543093560964)\n",
      "Entropy: 2.2848569671015464\n",
      "Expected Length: 2.316990224552212\n"
     ]
    }
   ],
   "source": [
    "# Construct the tree\n",
    "encoding_tree = build_huffman_tree(nucleotide_frequencies)\n",
    "\n",
    "# Generate Huffman codes\n",
    "codes = codes_from_tree(encoding_tree)\n",
    "\n",
    "# Print results\n",
    "print(\"Symbol Probabilities:\", nucleotide_frequencies)\n",
    "print(\"Huffman Codes:\", codes)\n",
    "print(\"Huffman tree from weights:\")\n",
    "print_tree(encoding_tree)\n",
    "print(\"Entropy:\", entropy(nucleotide_frequencies))\n",
    "print(\"Expected Length:\", expected_length(codes, nucleotide_frequencies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286fa98",
   "metadata": {},
   "source": [
    "As you can see in this case expected length is a bit larger than the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc00ac",
   "metadata": {},
   "source": [
    "### Compress the fasta file\n",
    "\n",
    "Now we can compress the fasta file using the generated Huffman codes. We will also create a header that contains all necessary information to decode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e0aa7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed file size: 13528355 bytes\n"
     ]
    }
   ],
   "source": [
    "# Generate genome encoding\n",
    "genome_bits_encoding = \"\".join([codes.get(char) for char in genome])\n",
    "genome_bytes_encoding = bits2bytes(genome_bits_encoding)\n",
    "\n",
    "# Generate the header\n",
    "code_lengths = {symbol: len(code) for symbol, code in codes.items()}\n",
    "header = write_header(code_lengths, len(genome_bits_encoding))\n",
    "\n",
    "# Write the compressed data to a file\n",
    "with open(\"genome.fa.huff\", \"wb\") as f:\n",
    "    f.write(header)\n",
    "    f.write(genome_bytes_encoding)\n",
    "\n",
    "print(f\"Compressed file size: {os.path.getsize('genome.fa.huff')} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b8ad0",
   "metadata": {},
   "source": [
    "For comparison let's remove the header and new line characters from original fasta file and compare sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a7a226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 claptar claptar  46M Jan 30  2025 genome.fa\n",
      "-rw-r--r-- 1 claptar claptar  13M Aug 31 18:51 genome.fa.huff\n",
      "-rw-r--r-- 1 claptar claptar  45M Aug 31 18:51 genome_clean.fa\n",
      "-rw-r--r-- 1 claptar claptar  45M Aug 31 18:33 genome_no_header_no_newlines.fa\n",
      "-rw-r--r-- 1 claptar claptar 9.5M Aug 31 18:50 genome_no_header_no_newlines.fa.bz2\n",
      "-rw-r--r-- 1 claptar claptar 9.5M Aug 31 18:45 genome_no_header_no_newlines.fa.bz9\n",
      "-rw-r--r-- 1 claptar claptar  11M Aug 31 18:50 genome_no_header_no_newlines.fa.gz\n",
      "-rw-r--r-- 1 claptar claptar 3.8M Aug 31 18:50 genome_no_header_no_newlines.fa.gz9\n",
      "-rw-r--r-- 1 claptar claptar  21M Aug 31 18:50 genome_no_header_no_newlines.fa.lz4\n",
      "-rw-r--r-- 1 claptar claptar 2.4M Aug 31 18:50 genome_no_header_no_newlines.fa.xz\n",
      "-rw-r--r-- 1 claptar claptar 8.2M Aug 31 18:46 genome_no_header_no_newlines.fa.xz9\n",
      "-rw-r--r-- 1 claptar claptar  11M Aug 31 18:50 genome_no_header_no_newlines.fa.zst\n",
      "-rw-r--r-- 1 claptar claptar    0 Aug 31 18:46 genome_no_header_no_newlines.fa.zst19\n"
     ]
    }
   ],
   "source": [
    "!tail -n +2 genome.fa | tr -d '\\n' > genome_clean.fa\n",
    "!ls -lh genome*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55e3de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FASTA file size: 47488540 bytes\n",
      "Original FASTA file size (no header, no newlines): 46709983 bytes\n",
      "Compressed file size: 13528355 bytes\n",
      "Compression ratio: 46709983 / 13528355 = 3.45\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original FASTA file size: {os.path.getsize('genome.fa')} bytes\")\n",
    "print(f\"Original FASTA file size (no header, no newlines): {os.path.getsize('genome_clean.fa')} bytes\")\n",
    "print(f\"Compressed file size: {os.path.getsize('genome.fa.huff')} bytes\")\n",
    "print(f\"Compression ratio: {os.path.getsize('genome_clean.fa')} / {os.path.getsize('genome.fa.huff')} = {os.path.getsize('genome_clean.fa') / os.path.getsize('genome.fa.huff'):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1ca59",
   "metadata": {},
   "source": [
    "As you can see, the compressed file is significantly smaller than the original FASTA file, even after removing the header and newlines. This demonstrates the effectiveness of the Huffman coding compression technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee2990",
   "metadata": {},
   "source": [
    "### Compare compression rates to popular tools\n",
    "\n",
    "Here's a brief description of each compression tool and their relationship to Huffman coding:\n",
    "\n",
    "#### Standard Compression Tools\n",
    "##### gzip (.gz)\n",
    "\n",
    "- Uses DEFLATE algorithm which combines LZ77 + Huffman coding\n",
    "- Fast compression, widely supported\n",
    "- Uses Huffman coding for the final encoding step\n",
    "##### bzip2 (.bz2)\n",
    "\n",
    "- Uses Burrows-Wheeler Transform + Move-to-Front + Huffman coding\n",
    "- Better compression than gzip, slower speed\n",
    "- Uses Huffman coding as the final entropy coding stage\n",
    "##### xz (.xz)\n",
    "\n",
    "- Uses LZMA2 algorithm with range encoding (arithmetic coding variant)\n",
    "- Excellent compression ratio, slower than gzip/bzip2\n",
    "- Does NOT use Huffman coding - uses arithmetic coding instead\n",
    "##### lz4 (.lz4)\n",
    "\n",
    "- Uses LZ4 algorithm focused on speed over compression ratio\n",
    "- Very fast compression/decompression\n",
    "- Does NOT use Huffman coding - uses simple byte-oriented encoding\n",
    "##### zstd (.zst)\n",
    "\n",
    "- Modern algorithm using LZ77 + FSE (Finite State Entropy)\n",
    "- Good balance of speed and compression ratio\n",
    "- Does NOT use Huffman coding - uses FSE (similar to ANS coding)\n",
    "\n",
    "\n",
    "#### Summary\n",
    "- Use Huffman coding: gzip, bzip2\n",
    "- Don't use Huffman coding: xz (arithmetic), lz4 (simple), zstd (FSE)\n",
    "\n",
    "Our Huffman implementation will be most comparable to gzip and bzip2 since they also use Huffman coding in their compression pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ec4bc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46M\tgenome.fa\n",
      "13M\tgenome.fa.huff\n",
      "45M\tgenome_clean.fa\n",
      "9.5M\tgenome_clean_high_compression.fa.bz9\n",
      "9.9M\tgenome_clean_high_compression.fa.gz9\n",
      "8.2M\tgenome_clean_high_compression.fa.xz9\n",
      "8.3M\tgenome_clean_high_compression.fa.zst19\n",
      "9.5M\tgenome_clean_standart.fa.bz2\n",
      "11M\tgenome_clean_standart.fa.gz\n",
      "21M\tgenome_clean_standart.fa.lz4\n",
      "8.3M\tgenome_clean_standart.fa.xz\n",
      "11M\tgenome_clean_standart.fa.zst\n"
     ]
    }
   ],
   "source": [
    "# Standard compression tools\n",
    "!gzip -c genome_clean.fa > genome_clean_standart.fa.gz\n",
    "!bzip2 -c genome_clean.fa > genome_clean_standart.fa.bz2\n",
    "!xz -c genome_clean.fa > genome_clean_standart.fa.xz\n",
    "!lz4 -c genome_clean.fa > genome_clean_standart.fa.lz4\n",
    "!zstd -c genome_clean.fa > genome_clean_standart.fa.zst\n",
    "\n",
    "# High compression variants\n",
    "!gzip -9 -c genome_clean.fa > genome_clean_high_compression.fa.gz9\n",
    "!bzip2 -9 -c genome_clean.fa > genome_clean_high_compression.fa.bz9\n",
    "!xz -9 -c genome_clean.fa > genome_clean_high_compression.fa.xz9\n",
    "!zstd -19 -c genome_clean.fa > genome_clean_high_compression.fa.zst19\n",
    "\n",
    "# Compare sizes in human readable format\n",
    "!du -h genome*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d4a4e",
   "metadata": {},
   "source": [
    "### Decode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
