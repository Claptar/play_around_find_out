{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e2bc08",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda5709",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f78e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d247e",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fc59d",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "This notebook is inpired by information theory [lectures by David McKay](https://www.youtube.com/playlist?list=PLN3p8NUNcClDu1hc2m5cVp8FOEmuF3vRy). I want to implement [Huffman coding algorithm](https://en.wikipedia.org/wiki/Huffman_coding) for data compression and use it to compress [human genome](https://en.wikipedia.org/wiki/Human_genome) sequence. Why doing it? Well, I'm greately facinated by [Arithmetic coding](https://en.wikipedia.org/wiki/Arithmetic_coding) algorithm for data compression and Huffman's algorithm would be great as a beseline to compare to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da187cfc",
   "metadata": {},
   "source": [
    "## Brief overview of related concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4f91c",
   "metadata": {},
   "source": [
    "### Symbol coding problem\n",
    "\n",
    "In symbol coding problem we have a set of symbols $S = \\{s_1, s_2, \\ldots, s_n\\}$ with probabilities $P = \\{p_1, p_2, \\ldots, p_n\\}$ and we want to encode them using binary strings (codewords) $C = \\{c_1, c_2, \\ldots, c_n\\}$ such that the expected codeword length is minimized. The expected codeword length $L$ is given by:\n",
    "\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i), \\tag{1}\n",
    "$$\n",
    "\n",
    "where $l(c_i)$ is the length of codeword $c_i$. Let's look at an example to illustrate this.\n",
    "\n",
    "---\n",
    "**Example 1:** Let's say we have a set of symbols $S = \\{A, B, C, D\\}$ with probabilities $P = \\{0.5, 0.25, 0.125, 0.125\\}$. We want to find the optimal codewords $C$ that minimize the expected codeword length.\n",
    "\n",
    "a) One possible solution is to assign the following codewords:\n",
    "- A: 1000\n",
    "- B: 0100\n",
    "- C: 0010\n",
    "- D: 0001\n",
    "\n",
    "The expected codeword length $L$ is calculated as follows:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i) = 0.5 \\cdot 4 + 0.25 \\cdot 4 + 0.125 \\cdot 4 + 0.125 \\cdot 4 = 4\n",
    "$$\n",
    "\n",
    "b) Another possible solution is to assign the following codewords:\n",
    "- A: 1\n",
    "- B: 01\n",
    "- C: 001\n",
    "- D: 0001\n",
    "\n",
    "The expected codeword length $L$ is calculated as follows:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i) = 0.5 \\cdot 1 + 0.25 \\cdot 2 + 0.125 \\cdot 3 + 0.125 \\cdot 4 = 1.875\n",
    "$$\n",
    "\n",
    "c) Third possible solution is to assign the following codewords:\n",
    "- A: 1\n",
    "- B: 00\n",
    "- C: 010\n",
    "- D: 10\n",
    "\n",
    "The expected codeword length $L$ is calculated as follows:\n",
    "$$\n",
    "L = \\sum_{i=1}^{n} p_i \\cdot l(c_i) = 0.5 \\cdot 1 + 0.25 \\cdot 2 + 0.125 \\cdot 3 + 0.125 \\cdot 4 = 1.625\n",
    "$$\n",
    "---\n",
    "\n",
    "Moreover we want our code to have several useful properties:\n",
    "- **Uniquely decodable**: for any string $x$ and $y$ such that $x \\neq y$ codewords $C(x)$ and $C(y)$ must be different $C(x) \\neq C(y)$. In plain english this means that we can always decode a string of codewords back to the original symbols without ambiguity. In this light, solution (c) in example 1 is not uniquely decodable because the both string `DC` and `ABD` are encoded as $C(DC)=C(ABD) = 10010$.\n",
    "- **Minimal expected codeword length**: we want to minimize the expected codeword length $L$.\n",
    "\n",
    "**Note:** [ASCII](https://en.wikipedia.org/wiki/ASCII) code is another interesting example of symbol coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665a49f",
   "metadata": {},
   "source": [
    "### Source coding theorem\n",
    "\n",
    "A question arises: what is the minimum expected codeword length $L$ that we can achieve? The answer is given by [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) which states that the minimum expected codeword length $L$ is bounded by the entropy $H$ of the source:\n",
    "\n",
    "$$\n",
    "L \\geq H, \\tag{2}\n",
    "$$\n",
    "\n",
    "where the entropy $H$ is defined as:\n",
    "\n",
    "$$\n",
    "H = -\\sum_{i=1}^{n} p_i \\log_2 p_i, \\tag{3}\n",
    "$$\n",
    "\n",
    "This means that no lossless compression scheme can achieve an expected codeword length less than the entropy of the source.\n",
    "\n",
    "By comparing equations (1) and (2) we can see that equality $L = H$ holds when codeword length is equal to $l(c_i) = -\\log_2 p_i$ for all symbols $s_i$. However, this is not always possible because codeword lengths must be integers. Therefore, we can only achieve $L$ that is close to $H$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
